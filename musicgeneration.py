# -*- coding: utf-8 -*-
"""MusicGeneration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X8YBzqOqyG0dw0hemPx8NWscjiSOtzrr
"""

!pip install comet_ml > /dev/null 2>&1
import comet_ml
COMET_API_KEY = 'B5lNGnSKgiSHGAjh9KxJuWFJF'
# Import PyTorch and other relevant libraries
import torch
import torch.nn as nn
import torch.optim as optim

# Download and import the MIT Introduction to Deep Learning package
!pip install mitdeeplearning --quiet
import mitdeeplearning as mdl

# Import all remaining packages
import numpy as np
import os
import time
import functools
from IPython import display as ipythondisplay
from tqdm import tqdm
from scipy.io.wavfile import write
!apt-get install abcmidi timidity > /dev/null 2>&1


assert torch.cuda.is_available(), "Please enable GPU from runtime settings"
assert COMET_API_KEY != "", "Please insert your Comet API Key"

# Download the dataset
songs = mdl.lab1.load_training_data()

# Print one of the songs to inspect it in greater detail!
example_song = songs[0]
print("\nExample song: ")
print(example_song)

# Convert the ABC notation to audio file and listen to it
mdl.lab1.play_song(example_song)

# Join our list of song strings into a single string containing all songs
songs_joined = "\n\n".join(songs)

# Find all unique characters in the joined string
vocab = sorted(set(songs_joined))
print("There are", len(vocab), "unique characters in the dataset")

### Define numerical representation of text ###

# Create a mapping from character to unique index.
# For example, to get the index of the character "d",
#   we can evaluate `char2idx["d"]`.
char2idx = {u: i for i, u in enumerate(vocab)}

# Create a mapping from indices to characters. This is
#   the inverse of char2idx and allows us to convert back
#   from unique index to the character in our vocabulary.
idx2char = np.array(vocab)

print('{')
for char, _ in zip(char2idx, range(20)):
    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))
print('  ...\n}')

def vectorize_string(string):
    '''Convert the string into a numeric vector using the char2idx mapping.'''
    return np.array([char2idx[char] for char in string])

# Now we vectorize the songs string
vectorized_songs = vectorize_string(songs_joined)

print ('{} ---- characters mapped to int ----> {}'.format(repr(songs_joined[:10]), vectorized_songs[:10]))
# check that vectorized_songs is a numpy array
assert isinstance(vectorized_songs, np.ndarray), "returned result should be a numpy array"

def get_batch(vectorized_songs, seq_length, batch_size):
    # the length of the vectorized songs string
    n = vectorized_songs.shape[0] - 1

    # randomly choose the starting indices for the examples in the training batch
    idx = np.random.choice(n - seq_length, batch_size)

    # Construct a list of input sequences for the training batch
    input_batch = [vectorized_songs[i:i + seq_length] for i in idx]

    # Construct a list of output sequences for the training batch
    output_batch = [vectorized_songs[i + 1:i + seq_length + 1] for i in idx]

    # Convert the input and output batches to tensors
    x_batch = torch.tensor(input_batch, dtype=torch.long)
    y_batch = torch.tensor(output_batch, dtype=torch.long)

    return x_batch, y_batch

# Perform some simple tests to make sure your batch function is working properly!
test_args = (vectorized_songs, 10, 2)
x_batch, y_batch = get_batch(*test_args)
assert x_batch.shape == (2, 10), "x_batch shape is incorrect"
assert y_batch.shape == (2, 10), "y_batch shape is incorrect"
print("Batch function works correctly!")

x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)

for i, (input_idx, target_idx) in enumerate(zip(x_batch[0], y_batch[0])):
    print("Step {:3d}".format(i))
    print("  input: {} ({:s})".format(input_idx, repr(idx2char[input_idx.item()])))
    print("  expected output: {} ({:s})".format(target_idx, repr(idx2char[target_idx.item()])))

# ======================
# 1. SETUP AND INITIALIZATION
# ======================

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Hyperparameters
params = dict(
    num_training_iterations=5000,
    batch_size=32,
    seq_length=100,
    learning_rate=3e-3,
    embedding_dim=256,
    hidden_size=1024,
    num_layers=2,
    grad_clip=1.0,
    weight_decay=1e-5
)

# ======================
# 2. MODEL DEFINITION
# ======================

class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=2):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Layers
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x, hidden=None):
        batch_size = x.size(0)

        # Embed input
        x = self.embedding(x)

        # Initialize hidden state if not provided
        if hidden is None:
            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
            hidden = (h0, c0)

        # LSTM forward pass
        out, hidden = self.lstm(x, hidden)

        # Reshape and project to vocabulary
        out = out.contiguous().view(-1, self.hidden_size)
        out = self.fc(out)

        return out, hidden

# ======================
# 3. TRAINING UTILITIES
# ======================

def compute_loss(labels, logits):
    """Compute cross-entropy loss"""
    labels = labels.view(-1)
    logits = logits.view(-1, logits.size(-1))
    return nn.CrossEntropyLoss()(logits, labels)

def train_step(x, y, model, optimizer):
    """Single training step"""
    model.train()
    optimizer.zero_grad()

    # Forward pass
    logits, _ = model(x)
    loss = compute_loss(y, logits)

    # Backward pass with gradient clipping
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), params['grad_clip'])

    # Update weights
    optimizer.step()

    return loss.item()

# ======================
# 4. INITIALIZE COMPONENTS
# ======================

# Model
model = LSTMModel(
    vocab_size=len(vocab),
    embedding_dim=params['embedding_dim'],
    hidden_size=params['hidden_size'],
    num_layers=params['num_layers']
).to(device)

# Optimizer
optimizer = optim.Adam(
    model.parameters(),
    lr=params['learning_rate'],
    weight_decay=params['weight_decay']
)

# Experiment tracking
experiment = comet_ml.Experiment(
    api_key=COMET_API_KEY,
    project_name="6S191_Lab1_Part2"
)
for param, value in params.items():
    experiment.log_parameter(param, value)

# Checkpoint directory
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "my_ckpt")
os.makedirs(checkpoint_dir, exist_ok=True)

# ======================
# 5. TRAINING LOOP
# ======================

history = []
plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')

for iter in tqdm(range(params["num_training_iterations"]), desc="Training"):
    # Get batch
    x_batch, y_batch = get_batch(vectorized_songs, params["seq_length"], params["batch_size"])

    # Move to device
    x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)
    y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)

    # Training step
    loss = train_step(x_batch, y_batch, model, optimizer)

    # Log metrics
    history.append(loss)
    experiment.log_metric("loss", loss, step=iter)
    plotter.plot(history)

    # Save checkpoint
    if iter % 100 == 0:
        torch.save(model.state_dict(), checkpoint_prefix + f"_iter{iter}.pt")

# Final save
torch.save(model.state_dict(), checkpoint_prefix + "_final.pt")
experiment.flush()

def generate_text(model, start_string, generation_length=1000, temperature=0.8):
    # Convert the start string to numbers
    input_idx = [char2idx[char] for char in start_string]
    input_idx = torch.tensor([input_idx], dtype=torch.long).to(device)

    # Initialize hidden state
    hidden = None

    # List to store generated characters
    text_generated = []

    # Clear progress bars
    if hasattr(tqdm, '_instances'):
        tqdm._instances.clear()

    for i in tqdm(range(generation_length), desc="Generating"):
        # Get predictions - your model returns (logits, hidden)
        logits, hidden = model(input_idx, hidden)

        # Get last prediction (since we're feeding one character at a time)
        last_logits = logits[-1, :]  # Get last output

        # Apply temperature scaling
        scaled_logits = last_logits / temperature
        probs = torch.softmax(scaled_logits, dim=-1)

        # Sample from distribution
        predicted_id = torch.multinomial(probs, num_samples=1)

        # Update input for next iteration
        input_idx = predicted_id.unsqueeze(0)

        # Store result
        text_generated.append(idx2char[predicted_id.item()])

    return start_string + ''.join(text_generated)

# Generate ABC format music with 1000 characters
generated_text = generate_text(
    model,                # Your trained LSTM model
    start_string="X:",    # Standard ABC notation header
    generation_length=1000  # Number of characters to generate
)

# Print the first 500 characters to preview
print("Generated music preview:")
print(generated_text[:500] + "...")

# Save the generated text to a file
with open("generated_song.abc", "w") as f:
    f.write(generated_text)
print("\nSaved generated music to 'generated_song.abc'")

# Play the generated songs (if play_song function exists)
if 'mdl' in globals() and hasattr(mdl.lab1, 'play_song'):
    generated_songs = mdl.lab1.extract_song_snippet(generated_text)
    for i, song in enumerate(generated_songs):
        print(f"\nPlaying generated song {i+1}:")
        waveform = mdl.lab1.play_song(song)
        if waveform:
            ipythondisplay.display(waveform)

### Play back generated songs ###

generated_songs = mdl.lab1.extract_song_snippet(generated_text)

for i, song in enumerate(generated_songs):
  # Synthesize the waveform from a song
  waveform = mdl.lab1.play_song(song)

  # If its a valid song (correct syntax), lets play it!
  if waveform:
    print("Generated song", i)
    ipythondisplay.display(waveform)

    numeric_data = np.frombuffer(waveform.data, dtype=np.int16)
    wav_file_path = f"output_{i}.wav"
    write(wav_file_path, 88200, numeric_data)

    # save your song to the Comet interface -- you can access it there
    experiment.log_asset(wav_file_path)

# when done, end the comet experiment
experiment.end()